The motivation of network representation learning, is almost same as in NLP. Besides, though our materials, you would notice that its application shares same characters (mostly in finding similar objects, understand the relationship of vertices)


Social network analysis is my love, I know this area when I was doing a [school research project](https://github.com/zihaolucky/Undergraduate-Innovation-Program). Then I got an intern offer from [Zhihu](http://www.zhihu.com), where I study the relationship of topics. I built a co-occurrence topic network and run some modularity-based algorithm (community detection), finally we construct a hybrid of network and tree structure. It's easy to visualize, with Gephi or some other network visualization tools, easy to understand and evaluate by our domain expert. But I was still wondering if there exist any way we could quantify those objects in network research.

Anyway, I think these works are cool and valuable. As Zhihu has some methods to quantify ones expertise under some topics, but sometimes it's sparse. We can use this topic network, to infer his/her expertise with some label-propagation model.


Back to our course, recently I read some interesting papers about network representation learning, matrix factorization and word2vec. They share some commons, I would like to share with you.


### Objective

* DeepWalk. We start from here, learn the basic idea of turning word2vec into network representation learning.

* Matrix factorization for network research. We would see the relation of DeepWalk and MF, and some other interesting works in this area.


